<!DOCTYPE html>

<html lang="ru">

<head>
    <meta charset="utf-8">
    <title>Задание 1</title>
</head>

<body>
    <header>
        <h1>В поисках свежести</h1>
    </header>
    <main>
        <article>
            <p><time datetime="2010-03-20">20 марта 2010 года</time> началось извержение вулкана Эйяфьядлайёкюдль в
                Исландии. </p>
            <p><time datetime="2015-07-14">14 июля 2015 года</time> межпланетная станция <a
                    href="https://science.nasa.gov/mission/new-horizons/"
                    title="Официальная информация о станции от NASA">New Horizons</a> передала на Землю фотографии
                Плутона.</p>
            <p> <time datetime="2019-04-15">15 апреля 2019 года</time> случился пожар в соборе Парижской
                Богоматери.</p>
            <section>
                <h4>Что общего в этих случаях?</h4>
                <p>Каждое подобное событие сопровождается <em>всплеском интереса</em> со стороны пользователей
                    интернета.
                    Люди хотят не только прочитать о том, что произошло, но и взглянуть на фотографии. Они идут в
                    <strong>поиск
                        картинок</strong> и ожидают найти там свежие, актуальные снимки, которые могли не существовать
                    ещё
                    несколько часов назад. Интерес возникает неожиданно и за несколько дней падает почти до нуля.
                </p>
                <p>Особенность ситуации в том, что обычные поисковые механизмы не заточены на подобный сценарий. Более
                    того,
                    критерий свежести контента противоречит другим важным свойствам хорошего поиска: релевантности,
                    авторитетности и т. д. Нужны особые технологии, чтобы не просто находить новый контент, но и
                    сохранять
                    баланс в результатах.</p>
                <p>Меня зовут <strong><a href="https://habr.com/ru/users/imgbase/" title="Профиль Дениса на Хабре">Денис
                            Сахнов</a></strong>, сегодня я расскажу о новом подходе к доставке свежего
                    контента
                    до
                    <a href="https://yandex.ru/images">Яндекс.Картинок</a>. А мой коллега <strong>Дмитрий
                        Кривоконь</strong> <a href="https://habr.com/ru/users/krivokon/">@krivokon</a> поделится
                    подробностями
                    о
                    метриках и
                    ранжировании свежих картинок. Вы узнаете о старом и новом подходе к оценке качества. А ещё мы
                    напомним о
                    <abbr title="Yandex Tables">YT</abbr>, Logbroker и <abbr title="Real-Time Map Reduce">RTMR</abbr>.
                <p>Чтобы поиск картинок работал хорошо на той части запросов, ответы на которые должны содержать
                    <em>свежий</em> контент, нужно решить следующие задачи:
                <nav>
                    <ol>
                        <li><a href="#section1">Научиться быстро <strong>находить</strong> и <strong>скачивать</strong>
                                свежие
                                картинки.</a></li>
                        <li><a href="#section2">Научиться быстро их <strong>обрабатывать</strong>.</a></li>
                        <li><a href="#section3">Научиться быстро <strong>собирать документы</strong> для поиска на базе
                                картинок
                                (этот пункт станет понятнее по ходу
                                повествования).</a></li>
                        <li><a href="#section4">Сформулировать <strong>критерии качества поиска</strong> свежего
                                контента.</a>
                        </li>
                        <li><a href="#section5">Научиться <strong>ранжировать</strong> и <strong>смешивать</strong>
                                контент в
                                выдаче, исходя из требований качества.</a></li>
                    </ol>
                </nav>
                </p>
                <p>Начнем с первого пункта.</p>
            </section>
            <section id="section1"></section>
            <h2>1. Добываем картинки</h2>
            <p>В интернете множество сайтов, многие из них что-то регулярно публикуют, в том числе картинки. Чтобы люди
                увидели всё это в поиске Картинок, робот должен дойти до сайта и скачать контент. <br>Обычно поиск так и
                работает: мы относительно быстро обходим известные нам сайты и получаем новые картинки. Но когда речь
                идёт о
                контенте, который вдруг становится актуальным прямо сейчас, эта модель не справляется. Потому что
                интернет
                огромный, невозможно «прямо сейчас» скачать HTML-документы всех сайтов в мире и быстро всё это
                переварить.
                По крайней мере никто в мире такую задачу ещё не решил.Кто-то может представить себе решение проблемы
                таким
                образом: отслеживать всплески запросов и в первую очередь обрабатывать только те источники, которые
                как-то
                соответствуют запросам. Но это хорошо звучит только на бумаге.</p>
            <ul>
                <li>Во-первых, чтобы проверить соответствие чего-то чему-то, нужно <em>уже иметь на руках контент.</em>
                </li>
                <li>Во-вторых, если мы начинаем что-то делать <em>после</em> пика запросов, то мы уже <em>опоздали</em>.
                    <br>Как бы дико это ни
                    звучало, нужно находить свежий контент <strong>до того</strong>, как в нём возникла потребность.
                </li>
            </ul>
            <h4>Но как предсказать неожиданное?</h4>
            <p>Правильный ответ: <em>никак</em>. Мы ничего не знаем о графике извержений вулканов. Но мы знаем, на каких
                сайтах
                обычно появляется свежий и полезный контент. С этой стороны мы и пошли. Мы стали применять
                <strong>машиннообученную формулу</strong>, которая <em>приоритизирует обход</em> нашего робота в
                зависимости
                от качества и актуальности контента.
            </p>
            <p>Да простят нас сеошники: в детали тут углубляться не будем. Задача робота — как можно <strong>быстрее
                    доставить</strong> до
                нас HTML-документы. Только после этого мы можем взглянуть на их начинку и найти там новые тексты, ссылки
                на
                картинки и т. п.</p>
            <p>Ссылки на картинки — это хорошо, но пока что не особо полезно для поиска. Их в первую очередь нужно
                <em>скачать</em> к
                нам. Но новых ссылок на картинки опять же слишком много, чтобы скачать их мгновенно. И проблема тут не
                только в наших ресурсах: владельцы сайтов тоже не хотели бы, чтобы Яндекс их случайно заддосил. Поэтому
                мы
                используем машинное обучение для <strong>приоритизации скачивания картинок</strong>. Факторы разные, их
                много, всё
                объяснять не будем, но для примера можем сказать, что <em>частота</em>, с которой картинка появляется на
                разных
                ресурсах, тоже влияет на приоритет.
            </p>
            <p>Теперь у нас есть <em>список ссылок на картинки</em>. Дальше мы их скачиваем к себе. При этом используем
                собственный сервис <strong>Logbroker</strong>. Эта штука выступает в качестве транспортной шины, успешно
                переживающей
                огромные объёмы трафика. Несколько лет назад наш коллега <em><a
                        href="https://habr.com/ru/users/aozeritsky/" title="Профиль Алексея на Хабре">Алексей
                        Озерицкий</a></em> уже <a href="https://habr.com/ru/companies/yandex/articles/239823/"
                    title="Статья Алексея на Хабре">рассказывал</a> об
                этой
                технологии на Хабре. </p>
            <p>На этом первый этап логически завершился. Мы определились с источниками и успешно добыли какие-то
                картинки.
                Осталось совсем чуть-чуть: <em>научиться с ними работать.</em></p>
            </section>
            <section id="section2">
                <h2>2. Обрабатываем картинки</h2>
                <p>Сами по себе картинки, конечно, полезны, но их ещё нужно подготовить. Это происходит так:</p>
                <ul>
                    <li>В сервисе stateless-вычислений RTHub готовятся версии разных размеров. Это нужно для поиска, где
                        удобно
                        в результатах показывать миниатюры, а исходный контент отдавать с сайта-источника по клику.</li>
                    <li>Рассчитываются нейросетевые фичи. В офлайне (т. е. заранее, а не в момент ранжирования) на
                        машинках
                        с
                        GPU запускаются нейросетки, результатом работы которых будут векторы фич картинки. А также
                        вычисляются
                        значения полезных классификаторов: красивости, эстетичности, нежелательного контента и многие
                        другие.
                        Всё это нам ещё понадобится.</li>
                    <li>А затем с использованием посчитанной по картинке информации склеиваются дубликаты. Это важно:
                        пользователь вряд ли обрадуется поисковым результатам, в которых будут преобладать одни и те же
                        картинки. При этом они могут немного отличаться: где-то обрезали край, где-то добавили водяной
                        знак
                        и т.
                        д. Склейку дубликатов мы проводим в два этапа. Сначала происходит грубая кластеризация близких
                        картинок
                        с помощью нейросетевых векторов. При этом картинки в кластере могут даже не совпадать по смыслу,
                        но
                        это
                        позволяет распараллелить дальнейшую работу с ними. </li>
                </ul>
                Далее уже внутри каждого кластера склеиваем дубликаты через поиск опорных точек на картинках. Обратите
                внимание:
                нейросети отлично ищут похожие картинки, но для поиска полных дубликатов эффективнее менее «модные»
                инструменты;
                нейросетки могут перемудрить и увидеть «одинаковое в разном».Итак, к концу этого этапа у нас есть
                готовые
                картинки в разных вариантах, прошедшие через склейку дубликатов, с предпросчитанными нейросетевыми и
                прочими
                фичами. </li>
                </ol>
                </p>
                <p>Отдаём в ранжирование? Нет, ещё рано.
                </p>
            </section>
            <section id="section3">
                <h2>3. Собираем картинки в документы</h2>
                <p><strong>Документ</strong> — это наше название сущности, которая участвует в <em>ранжировании</em>.
                </p>
                <p>Со стороны пользователя это может выглядеть как <em>ссылка на страницу</em> (поиск по сайтам),
                    <em>картинка</em> (поиск картинок), <em>ролик</em> (поиск видео), <em>кофеварка</em> (поиск
                    товаров),
                    что-то ещё. Но внутри за каждой единицей в выдаче поиска скрывается целый букет разнородной
                    информации.
                </p>
                <p>В нашем случае — не только сама картинка, её нейросетевые и прочие фичи, но и <strong>сведения о
                        страницах</strong>, где она помещена, <strong>тексты</strong>, которые на этих страницах её
                    описывают, <strong>статистика</strong> поведения пользователей (например, клики по картинке). </p>
                <p>Всё вместе — это и есть <em>документ</em>. И, прежде чем перейти непосредственно к поиску, документ
                    нужно
                    <strong>собрать</strong>. И механизм формирования обычной поисковой базы картинок здесь не подходит.
                    <br><em>Основной вызов</em> в том, что разные компоненты документа формируются в <em>разное
                        время</em> и
                    в <em>разных местах</em>. <br>Сведения о страницах и текстах может загрузить тот же самый Logbroker,
                    но
                    не одновременно с картинками. Данные о поведении пользователей в реалтайме поступают через систему
                    обработки логов <abbr title="Real-Time MapReduce">RTMR</abbr>. И всё это хранится
                    <em>независимо</em> от
                    картинок.
                </p>
                <p>Чтобы собрать документ, нужно последовательно обойти разные источники данных.Для формирования
                    основной
                    поисковой базы картинок мы используем <strong>MapReduce</strong>. </p>
                <p>Это эффективный, надёжный способ работы с <em>огромными массивами данных</em>. Но для задачи свежести
                    он
                    <em>не подходит</em>: нам требуется очень быстро получать из хранилища все необходимые для
                    формирования
                    каждого документа данные, что не соответствует MapReduce.
                </p>
                <p>Поэтому в контуре свежести мы используем другой способ: разнородная информация приходит в систему
                    потоковой обработки данных <strong>RTRobot</strong>, которая использует <strong><abbr
                            title="Key-Value">KV</abbr>-хранилища</strong> для <em>синхронизации</em> разных потоков
                    обработки данных и отказоустойчивости. </p>
                <p>В контуре свежести в качестве <abbr title="Key-Value">KV</abbr>-хранилища мы используем
                    <strong>динамические таблицы</strong> на базе нашей системы <abbr title="Yandex Tables">YT</abbr>.
                    По
                    сути, это хранилище всего контента, который может нам понадобиться. С очень быстрым доступом. Именно
                    оттуда мы оперативно запрашиваем всё, что может пригодиться для поиска картинок, собираем документы
                    и с
                    помощью LogBroker’а передаём их на <em>поисковые серверы</em>, откуда подготовленные данные
                    добавляются
                    в <em>поисковую базу</em>.
                </p>
                <p>Благодаря отдельному контуру для работы со свежестью, который охватывает все этапы (от поиска
                    картинок в
                    сети до подготовки документов), нам удаётся обрабатывать сотни новых картинок в секунду и доставлять
                    их
                    до поиска в среднем за несколько минут с появления.Но довести картинки до поиска недостаточно. Нужно
                    суметь показать их в результатах поиска тогда, когда они <em>полезны</em>. И здесь мы переходим к
                    следующему этапу — <strong>к определению полезности</strong>. Передаю слово <em>Дмитрию
                        @krivokon</em>.
                </p>
            </section>
            <section id="section4">
                <h2>4. Измеряем качество</h2>
                <p>Общий подход к оптимизации качества поиска начинается с <strong>выбора метрики</strong>. </p>
                <p>В поиске картинок Яндекса вид метрики примерно такой:</p>
                <math display="block">
                    <munderover>
                        <mo>&sum;</mo>
                        <mn>
                            <mrow>
                                <mi>i</mi>
                                <mo>=</mo>
                                <mn>0</mn>
                            </mrow>
                        </mn>
                        <mn>n</mn>
                    </munderover>

                    <msub>
                        <mi>p</mi>
                        <mi>i</mi>
                    </msub>
                    <mo>&middot;</mo>
                    <mo fence="true" form="prefix">(</mo>
                    <msub>
                        <mi>r</mi>
                        <mi>i</mi>
                    </msub>
                    <mo>+</mo>
                    <mi>f</mi>
                    <mo fence="true" form="prefix">(</mo>
                    <msub>
                        <mi>w</mi>
                        <mi>i</mi>
                    </msub>
                    <mo>,</mo>
                    <mo>.</mo>
                    <mo>.</mo>
                    <mo>.</mo>
                    <mo>,</mo>
                    <msub>
                        <mi>m</mi>
                        <mi>i</mi>
                    </msub>
                    <mo fence="true" form="postfix">)</mo>
                    <mo fence="true" form="postfix">)</mo>

                </math>
                <p>где <em>n</em> — это <em>количество</em> первых картинок (документов) выдачи, которые мы оцениваем;
                </p>
                <p><em>p<sub>i</sub></em> — вес позиции в выдаче (чем выше позиция, тем больше вес);</p>
                <p><em>r<sub>i</sub></em>
                    — <em>релевантность</em> (насколько точно картинка соответствует запросу); </p>
                <p><em>w<sub>i</sub></em> …
                    <em>m<sub>i</sub></em> — прочие компоненты <em>качества ответа</em> (свежесть, красота, размер...);
                </p>
                <p><em>f(...)</em> — модель, которая <strong>агрегирует</strong> эти компоненты. Проще говоря, чем выше
                    в
                    выдаче будут более полезные картинки, тем больше сумма в этом выражении.
                </p>
                <p>Несколько слов о модели <em>f(...)</em>. Она обучается на <em>попарном сравнении картинок</em>
                    толокерами. Человек видит запрос и две картинки, а затем выбирает лучшую. Если повторить это
                    много-много
                    раз, то модель <strong>научится предсказывать</strong>, какой компонент качества наиболее важен для
                    конкретного запроса. К примеру, если запрос о свежих фотографиях чёрной дыры — то наибольший
                    коэффициент
                    у компонента свежести. А если о тропическом острове — то у красоты, потому что мало кто ищет
                    любительские фотографии страшненьких островов, обычно нужны именно привлекательные картинки. </p>
                <p><em>Чем визуально лучше выглядит выдача картинок в таких случаях, тем больше вероятность, что человек
                        продолжит пользоваться сервисом.</em></p>
                <p>Но не будем отвлекаться на это.Итак, задача алгоритмов ранжирования — <strong>оптимизировать</strong>
                    эту
                    метрику. Но нельзя оценивать все миллионы ежедневных запросов: это огромная нагрузка, причём в
                    первую
                    очередь на толокеров. Поэтому для контроля качества мы выделяем <em>случайную выборку</em> (корзину)
                    за
                    фиксированный промежуток времени. </p>
                <p>Казалось бы, есть метрика, в которой уже учтён компонент свежести, и есть выборка запросов для
                    контроля
                    качества. Можно на этом закрыть тему и перейти к ранжированию. Но нет.В случае свежих картинок
                    возникает
                    проблема. При оценке алгоритмов мы должны уметь понимать, что хорошо отвечаем на запрос пользователя
                    именно в тот момент, когда запрос и <strong>вводятся в поиске</strong>. Прошлогодний свежий запрос
                    может
                    не быть таковым сейчас. И хорошим ответом на него станет что-то другое. Поэтому фиксированная
                    корзина
                    запросов (например, за год) не подходит.</p>
                <p>В качестве первого подхода к решению этой проблемы мы попробовали обойтись совсем без корзины. По
                    определённой логике мы стали подмешивать в выдачу свежие картинки, а затем изучали поведение
                    пользователей. Если оно менялось в лучшую сторону (например, люди активнее кликали по результатам) —
                    значит, подмешивание было полезным. Но у такого подхода есть изъян: оценка качества напрямую
                    зависела от
                    качества наших алгоритмов. Например, если по какому-то запросу наш алгоритм не справится и не
                    подмешает
                    контент, то и сравнивать будет нечего, а значит, мы не поймём, нужен ли был там свежий контент. Так
                    мы
                    пришли к пониманию, что требуется <strong>независимая система оценки</strong>, которая будет
                    показывать
                    текущее качество работы наших алгоритмов и не зависеть от них. </p>
                <p>Второй наш подход заключался в следующем. Да, мы не можем использовать фиксированную корзину из-за
                    изменчивости свежих запросов. Но мы можем оставить за базу ту часть корзины, для которой <em>нет
                        требований по свежести</em>, а свежую часть доливать туда ежедневно. </p>
                <p>Для этого мы создали алгоритм, выделяющий в потоке пользовательских запросов те, на которые с высокой
                    вероятностью требуется ответ со свежими картинками. У таких запросов обычно неожиданно меняется
                    частность. Конечно же, мы используем <em>ручную валидацию</em>, чтобы отсеять шум и мусор и учесть
                    особые ситуации. К примеру, запрос может быть актуален только для определённой страны. В этом случае
                    нам
                    помогают уже не толокеры, а <strong>асессоры</strong>: такая работа требует особого опыта и знаний.
                </p>
                <p>При этом мы не только добавляем такие свежие запросы в корзину для оценки качества, но и
                    <strong>сохраняем результаты нашего поиска</strong> в момент обнаружения запросов. Так мы можем
                    оценить
                    не только первичное качество ответа, но и то, насколько быстро наш поиск <strong>среагировал на
                        событие</strong>.
                </p>
                <h4>Итак, подведём предварительные итоги. </h4>
                <p>Чтобы хорошо отвечать на свежие запросы, мы не только обеспечили быструю доставку до поиска и
                    обработку
                    картинок, но и <em>переизобрели способ оценки качества</em>. Осталось разобраться с тем, качество
                    чего
                    мы измеряем</p>
            </section>
            <section id="section5">
                <h2>5. Ранжируем</h2>
                <p>Напомню, выше мы описали переход от первого подхода к оценке качества поиска картинок ко второму: от
                    подмешивания результатов к ежедневному пополнению приёмочной корзины свежими запросами. </p>
                <p>Парадигма сменилась — понадобились изменения и самих алгоритмов. Это достаточно непросто объяснить
                    читателям со стороны, но я попробую. Если останутся вопросы — смело задавайте их в комментариях.</p>
                <p>Раньше методы были реализованы по аналогии с решением, о котором <a
                        href="https://habr.com/ru/companies/yandex/articles/329946/"
                        title="Статья Алексея на Хабре">рассказывал</a> наш коллега
                    <strong><a href="https://habr.com/ru/users/ashagraev/" title="Профиль Алексея на Хабре">Алексей
                            Шаграев</a></strong>. Есть <em>основной источник документов</em> (основной поисковый
                    индекс). А ещё
                    есть
                    дополнительный источник <strong>свежих</strong> документов, для которых критична скорость попадания
                    в
                    поиск.
                </p>
                <p>Документы из разных источников нельзя было ранжировать по единой логике, поэтому мы по достаточно
                    нетривиальной схеме подмешивали документы из свежего источника в основную выдачу. Далее сравнивали
                    метрики основной выдачи без дополнительных документов и с ними.Сейчас ситуация другая. Да, источники
                    по-прежнему физически разные, но с точки зрения метрик совершенно <em>неважно, откуда именно пришла
                        свежая картинка</em>. Она может и из основного источника попасть, если обычный робот успел до
                    неё
                    добраться. В этом случае метрики будут идентичны той ситуации, когда эта же картинка добралась до
                    выдачи
                    через отдельный источник.</p>
                <p> В новом подходе есть содержательная свежесть запроса и результата, а архитектура источников уже не
                    так
                    важна. В результате и <em>основные, и свежие документы ранжируются с помощью одной и той же
                        модели</em>,
                    что позволяет нам подмешивать свежие картинки в выдачу по существенно более простой логике, чем
                    раньше:
                    путём <strong>простой сортировки</strong> по значению на выходе единой модели. Конечно же, это
                    отразилось и на качестве. </p>
                <p>Идём дальше. Чтобы что-то отранжировать, нужен <strong>датасет</strong>, на котором модель будет
                    обучаться. Для свежих картинок — датасет с примерами <em>свежего</em> контента. У нас уже есть
                    основной
                    датасет, требовалось научиться добавлять в него примеры свежести. И тут мы вспоминаем о
                    <em>приёмочной
                        корзине</em>, которую уже используем для контроля качества.
                </p>
                <p>Свежие запросы в ней варьируются каждый день, а значит, уже на следующий день мы можем взять
                    <em>вчерашние</em> свежие запросы и <em>долить</em> их в датасет для обучения. При этом мы не
                    рискуем
                    переобучиться, так как одни и те же данные не используются одновременно для обучения и контроля.
                </p>
                <p>За счёт перехода на новую схему качество результатов поиска свежих картинок значительно возросло.
                    Если
                    раньше обучение основывалось в первую очередь на пользовательских статистиках на свежих запросах и у
                    нас
                    из-за этого возникала обратная связь с текущим алгоритмом ранжирования, то сейчас основой обучения
                    являются <strong>объективно собранные корзины запросов</strong>, которые зависят исключительно от
                    потока
                    пользовательских запросов. Это позволило нам научиться показывать свежие результаты даже тогда,
                    когда их
                    раньше не было. Кроме этого, за счёт объединения <strong>пайплайнов ранжирования</strong> основного
                    и
                    свежего контуров последний стал заметно быстрее развиваться (все улучшения в одном источнике теперь
                    автоматически доезжают и до второго).</p>
                <p>В одном посте невозможно подробно рассказать обо всей работе, которую проделала команда поиска
                    картинок
                    Яндекса. Надеемся, у нас получилось объяснить, в чём особенности поиска свежих картинок. И зачем
                    нужны
                    изменения на всех этапах поиска, чтобы пользователи смогли быстро найти свежие фотографии Плутона
                    или
                    любую другую актуальную информацию.</p>
            </section>
            <footer>
                <p>
                    Автор: <a href="https://habr.com/ru/users/imgbase/" title="Денис Сахнов">@imgbase</a> <br>
                    Разработчик<br>
                    Компания: <a href="https://yandex.ru/company">Яндекс</a></p>
            </footer>
        </article>
        <h2>Комментарии (3)</h2>
        <article>
            <p><a href="https://habr.com/ru/users/Kriminalist/">@Kriminalist</a> <time
                    datetime="2020-08-13T12:01"></time>13 августа 2020 в 12:01</p>
            <p>Нет уж, давайте как раз отвлечемся на именно этот момент.Фактически получается, что ни о какой
                персонализации выдачи речь не идет, поиск заточен под «обычно нужны», и у вас работает система с
                положительной обратной связью — чем больше выдачи вверху, тем выше рейтинг такой выдачи. Т.е.
                мейнстрим
                поглощает и подавляет «любительские фотографии страшненьких островов», а практической возможности
                выйти
                из этого потока у пользователя нет. И если мне нужны реальные фото без прикрас — я буду страдать.
            </p>
        </article>
        <article>
            <p><a href="https://habr.com/ru/users/imgbase/">@imgbase</a> <time datetime="2020-08-13T14:32"></time>13
                августа 2020 в 14:32 </p>
            <p>Запрос можно конкретизировать, чтобы помочь поиску понять потребности. Например, по запросу
                [любительские
                фотографии с островов] уже меньше «прикрас». Ещё важно понимать, что красота — это не синоним
                фотошопа.
                Даже любительские фотографии могут быть красивыми и удачными. И наоборот: постановочные могут не
                пройти
                по критерию привлекательности.</p>
        </article>
        <article>
            <p><a href="https://habr.com/ru/users/sumanai/">@sumanai</a> <time datetime="2020-08-13T13:52"></time>13
                августа 2020 в 13:52</p>
            <p>А зачем тогда нейросети? Неужели нейросети быстрее и меньше нагружают оборудование, нежели чем старые
                добрые алгоритмы?
            </p>
        </article>
    </main>
    <footer>
        <hr>
        <p>© Яндекс, <a href="mailto:help@yandex.ru">help@yandex.ru</a>,
        <address>Хохрякова, 10</address>
        </p>
    </footer>
</body>

</html>
